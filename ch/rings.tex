\chapter{Rings and modules}
\label{ch:rings}

\section{Rings and homomorphisms}

Recall that the hom-set, \(\Hom_{\Ab}(A, B)\), of two abelian groups \(G\) and
\(H\) is itself an abelian group under pointwise addition. In particular for any
abelian group \(G\), we have \(\Hom_{\Ab}(G, G) = \End_{\Ab}(G)\), the set of
all endomorphisms of \(G\). Now since \(\Ab\) is a category, we have a
well-defined composition law and thus the set \(\End_{\Ab}(G)\) is a monoid
under composition. Therefore two operations `naturally' arise and coexist on the
set \(\End_{\Ab}(G)\): the pointwise addition inherited from the abelian group
structure of \(\Hom_{\Ab}(G, G)\) and the composition inherited from the
category structure of \(\Ab\). This is a special case of an algebraic structure
known as a \emph{ring}, which we now formally define.

\begin{definition}
    A \emph{ring} is a triple \((R, +, \cdot)\), where \(R\) is a nonempty set
    and \(+\) and \(\cdot\) are two binary operations (often referred to as
    `addition' and `multiplication', respectively) such that:
    \begin{enumerate}[label=(\alph*)]
        \item \((R, +)\) is an abelian group.
        \item Multiplication is associative, i.e. \((a \cdot b) \cdot c = a
        \cdot (b \cdot c)\) for all \(a, b, c \in R\).
        \item Multiplication is distributive over addition, i.e. \(a \cdot (b +
        c) = a \cdot b + a \cdot c\) and \((a + b) \cdot c = a \cdot c + b \cdot
        c\) for all \(a, b, c \in R\).
        \item There exists a distinguished element \(1 \in R\) such that \(1
        \cdot a = a \cdot 1 = a\) for all \(a \in R\).
    \end{enumerate}
\end{definition}

More succinctly, axioms (b) and (d) above tell us that \((R, \cdot)\) is a
monoid with identity element \(1\).

Some texts may refer to the triple \((R, +, \cdot)\) satisfying the above axioms
as a \emph{ring with unity} or a \emph{ring with \(1\)} and only require that
the first three axioms be satisfied for \((R, +, \cdot)\) to be a ring. Rings
without unity are sometimes referred to as \emph{rngs} in the literature. For
example, the set of even integers \(2\Z\) with the usual operations of addition
and multiplication satisfies all the axioms above except for the requirement
that the multiplicative identity \(1\) be in \(2\Z\). Thus it is a `rng' by the
preceding definition. Nevertheless, we shall follow the above definition in this
text and all rings will be assumed to have a multiplicative identity unless
otherwise stated. If multiplication is commutative, i.e. \(a \cdot b = b \cdot
a\) for all \(a, b \in R\), then we say that \(R\) is a \emph{commutative ring}.
As in Chapter~\ref{ch:groups}, we will often write \(ab\) instead of \(a \cdot
b\).

Some elementary properties of rings derive from the properties of additive
groups and monoids. The identity element of the abelian group \((R, +)\) is
often denoted by \(0\), and is called the \emph{zero} of the ring \(R\). Since
\(R\) is an abelian group under addition, it follows that \(0\) must be unique,
as is the additive inverse \(-a\) of every element \(a \in R\). Moreover, for
all \(a, b \in R\), we have
\[
    -(-a) = a, \quad -0 = 0, \quad \text{and} \quad -(a + b) = (-a) + (-b).
\]
The usual laws for multiples of the form \(na\) where \(a \in R\) and \(n\) is
an integer also hold in rings. To wit, we have
\[
    (m+n)a = ma + na, \quad (-m)a = -(ma), \quad \text{and} \quad m(na) = (mn)a
\]
and because \(R\) is also abelian we have
\[
    m(a+b) = ma + mb
\]
for all \(a, b \in R\) and \(m, n \in \Z\). Because \(R\) is also a monoid under
multiplication, the distinguished element \(1 \in R\) (called the \emph{unity})
must likewise be unique and the usual laws of exponents must similarly hold, so
that we have, for all \(a \in R\) and \(m, n \in \Z\),
\[
    a^m a^n = a^{m+n} \quad \text{and} \quad (a^m)^n = a^{mn}.
\]

Other properties do not directly derive from \(R\) being an abelian group or a
monoid under addition and multiplication, respectively, but are consequences of
the distributive law. In particular we have the following results.

\begin{theorem}
    Let \(R\) be a ring. Then
    \begin{enumerate}[label={\normalfont(\alph*)}]
        \item \(0a = a0 = 0\) for all \(a \in R\);
        \item if \(R\) is not the zero ring, then \(0 \neq 1\);
        \item \((-a)b = a(-b) = -(ab)\) for all \(a, b \in R\);
        \item for all \(a, b \in R\),
        \[
            \left(\sum_{i=1}^{m} a_i\right)\left(\sum_{j=1}^{n} b_j\right) = \sum_{i=1}^{m} \sum_{j=1}^{n} a_i b_j;
        \]
        \item \((na)b = n(ab) = a(nb)\) for all \(a, b \in R\) and \(n \in \Z\);
        \item if \(R\) is commutative, then for all \(a, b \in R\) and \(n \in
        \Z\),
            \[
                (a + b)^n = \sum_{r = 0}^{n} \binom{n}{r} a^{n-r} b^r.
            \]
    \end{enumerate}
\end{theorem}

\begin{proof}\(\)
    \begin{enumerate}[label=(\alph*), wide]
        \item Since \(0\) is the additive identity, we have \(0a = (0 + 0)a = 0a
        + 0a\), whence \(0a = 0\). Similarly, \(a0 = a(0 + 0) = a0 + a0\), so
        \(a0 = 0\).

        \item If \(0 = 1\), then for all \(a \in R\), we have \(a = a1 = a0 =
        0\), from which it follows that \(R\) contains only a single element
        which is both the additive and multiplicative identity. We call this the
        \emph{zero ring}. As in the case of groups, we can consider any
        singleton \(\{*\}\) with addition and multiplication defined by \(* + *
        = *\) and \(* \cdot * = *\), respectively, to be the zero ring
        (analogous to the trivial group).
        
        \item From (a) above and the distributive law, we have
        \[
            0 = 0b = (a + (-a))b = ab + (-a)b,
        \]
        whence \((-a)b = -(ab)\). An analogous argument shows that \(a(-b) =
        -(ab)\).

        Since \(-(-a) = a\), we then have, as an immediate consequence, that
        \((-a)(-b) = a(-(-b)) = a(b) = ab\) and in particular \((-1)(-1) = 1\)
        and \((-1)a = -a\).

        \item For \(m = 1\), the proof is by induction on \(n\), with the case
        \(n = 2\) being left distribution. Having established this rule for \(m
        = 1\), a second induction on \(m\) completes the proof.
        
        \item For positive \(n\), the first equation is given by \(ab + \cdots +
        ab = (a + \cdots + a)b\) and is thus a special case of (d) above. For
        negative \(n\), we can use the result in (c).

        \item This result is often called the binomial theorem. First recall
        that
        \[
            \binom{n}{r} := \frac{n!}{r!(n-r)!}
        \]
        for all integers \(n\) and \(r\) with \(0 \leq r \leq n\). The proof is
        by induction on \(n\). For \(n = 0\), both sides of the equation are
        equal to \(1\). We can also verify that the result holds for \(n = 1\)
        and \(n = 2\). Now suppose that the result holds for some \(k \geq 2\),
        i.e.,
        \[
            (a + b)^k = \sum_{r = 0}^{k} \binom{k}{r} a^{k-r} b^r.
        \]
        We then have
        \begin{align*}
            (a + b)^{k+1} &= (a + b)(a + b)^k\\
            &= (a + b)\sum_{r = 0}^{k} \binom{k}{r} a^{k-r} b^r\\
            &= \sum_{r = 0}^{k} \binom{k}{r} a^{k-r+1} b^r + \sum_{r = 0}^{k} \binom{k}{r} a^{k-r} b^{r+1}\\
            &= \sum_{r = 0}^{k} \binom{k}{r} a^{k-r+1} b^r + \sum_{r = 1}^{k+1} \binom{k}{r-1} a^{k-r+1} b^r\\
            &= \binom{k}{0} a^{k+1} + \sum_{r = 1}^{k} \left(\binom{k}{r} + \binom{k}{r-1}\right) a^{k-r+1} b^r + \binom{k}{k} b^{k+1}\\
            &= a^{k+1} + \sum_{r = 1}^{k} \binom{k+1}{r} a^{k-r+1} b^r + b^{k+1} {\small \text{  (by Pascal's identity)}}\\
            &= \sum_{r = 0}^{k+1} \binom{k+1}{r} a^{k+1-r} b^r,
        \end{align*}
        thereby completing the induction step and establishing the result for
        all \(n \in \N\).
    \end{enumerate}
\end{proof}

We now look at some examples of rings and introduce some terminology.

\begin{example}[Boolean rings]
    Consider two subsets \(A\) and \(B\) of a set \(X\) and recall that the
    symmetric difference of \(A\) and \(B\) is defined by
    \[
        A \triangle B := (A \setminus B) \cup (B \setminus A).
    \]
    Let \(\Power{X}\) be the power set of \(X\) and define the operations of
    addition and multiplication on \(\Power{X}\) by (a) \(A + B := A \triangle
    B\) and (b) \(AB := A \cap B\). The empty set \(\emptyset\) is the zero
    element, since \(\emptyset + A = A + \emptyset = A\); and each subset \(A\)
    is its own inverse since \(A + A = A \triangle A = \emptyset\). The unity is
    the set \(X\) itself, since \(X A = A X = A\). Associativity of addition and
    multiplication is inherited from the set operations, and the distributive
    law holds since
    \[
        A (B + C) = A \cap (B \triangle C) = (A \cap B) \triangle (A \cap C) = (A B) + (A C).
    \]
    Thus \(\Power{X}\) is a ring under the operations of symmetric difference
    and intersection, and is called the \emph{Boolean ring} of subsets of \(X\).
    The Boolean ring is commutative since both addition and multiplication are
    commutative in \(\Power{X}\). Other set operations can be expressed in terms
    of the operations of the Boolean ring. In particular, the union of two sets
    \(A\) and \(B\) is given by \(A + B + AB\), and the complement of a set \(X
    \setminus A\) is given by \(1 + A\). The familiar De Morgan's law
    \[
        (A \cup B)^c = A^c \cap B^c
    \]
    (where \(A^c = X\setminus A\)) can be expressed, for example, in terms of
    the operations of the Boolean ring as
    \[
        1 + A + B + AB = (1 + A)(1 + B).
    \]
\end{example}

\begin{example}[Integral domains]
    \label{ex:integral-domains}
    Rings generalize our notion of number systems. In particular, the familiar
    sets \(\Z\), \(\Q\), \(\R\) and \(\C\) are all commutative rings under the
    usual operations of addition and multiplication. 

    The additive group \(\Z/n\Z\) of integers modulo \(n\) is a ring under the
    usual addition modulo \(n\) and multiplication modulo \(n\) which we define
    as \([a] \cdot [b] = [ab]\). Observe however that while additive
    cancellation holds in both \(\Z\) and \(\Z/n\Z\), multiplicative
    cancellation does not hold in \(\Z/n\Z\). For example, in \(\Z/6\Z\), we
    have \([2] \cdot [3] = [0] = [2] \cdot [0]\) but \([3] \neq [0]\). This is
    because the element \([3]\) is a zero divisor in \(\Z/6\Z\), i.e., there
    exists a nonzero element \([a] \in \Z/6\Z\) such that \([3] \cdot [a] =
    [0]\). In general, an element \(a \in R\) is a left (resp. right) \emph{zero
    divisor} if there exists a nonzero element \(b \in R\) such that \(ab = 0\)
    (resp. \(ba = 0\)). An element that is both a left and right zero divisor is
    simply called a (two-sided) \emph{zero divisor}. In any nonzero ring \(R\),
    the zero element is trivially a zero divisor. We say that a (left, right,
    two-sided) zero divisor is \emph{nontrivial} if it is not the zero element
    of \(R\).

    A nonzero commutative ring \(R\) is called an \emph{integral domain} if it
    has no nontrivial zero divisors. In other words, \(R\) is an integral domain
    if for all \(a, b \in R\), \(ab = 0\) implies that either \(a = 0\) or \(b =
    0\). This condition guarantees that multiplicative cancellation also holds
    in \(R\). Indeed if \(ab = ac\) for some \(a \neq 0\) in an integral domain
    \(R\), then \(ab - ac = a(b - c) = 0\); but since \(a \neq 0\) and \(R\) is
    an integral domain, we must have \(b - c = 0\) and thus \(b = c\). The
    familiar sets \(\Z\), \(\Q\), \(\R\) and \(\C\) are all integral domains but
    \(\Z/n\Z\) is an integral domain if and only if \(n\) is a prime number.
    Indeed if \(n = ab\) for some integers \(a, b\) with \(1 < a, b < n\), then
    \([a] \cdot [b] = [ab] = [n] = [0]\) and \([a] \neq [0]\) and \([b] \neq
    [0]\), so \(\Z/n\Z\) is not an integral domain. On the other hand if \(n =
    p\) is a prime number, so that for all integers \(a, b\) with \(1 < a, b <
    p\), \([ab] = [0]\) implies that \([a] = [0]\) or \([b] = [0]\), and thus
    \(\Z/p\Z\) is an integral domain.

    Alternatively, for any ring \(R\), an element \(a \in R\) is \emph{not} a
    left (resp. right) zero divisor if and only if the left (resp. right)
    multiplication map \(R \to R\) defined by \(x \mapsto ax\) (resp. \(x
    \mapsto xa\)) is injective. Verifying the left case (with the right case
    being analogous), if \(a \neq 0\) is not a left zero divisor, we have for
    any \(b, c \in R\) such that \(ab = ac\), that \(ab - ac = a(b - c) = 0\),
    so that if \(a \neq 0\) and \(a\) is not a left zero divisor, then \(b = c\)
    and left multipication by \(a\) is injective. (This is essentially the same
    argument in the previous paragraph establishing multiplicative cancellation
    in an integral domain.) Conversely, if \(a\) is a left zero divisor, then
    there exists a nonzero element \(b \in R\) such that \(ab = 0 = a \cdot 0\),
    so that the left multiplication map \(x \mapsto ax\) is not injective. 
\end{example}

\begin{example}[Units, division rings, fields]
    An element \(u\) of a ring \(R\) is a left (resp. right) \emph{unit} if
    there exists an element \(v \in R\) such that \(uv = 1\) (resp. \(vu = 1\)).
    The element \(v\) is called a right (resp. left) inverse of \(u\). An
    element that is both a left and right unit is simply called a (two-sided)
    \emph{unit}. If \(u\) is a two-sided unit in a ring \(R\) with a left
    inverse \(v\) and a right inverse \(w\), then we have
    \[
        vu = 1 \implies vuw = w \implies v(uw) = w \implies v = w
    \]
    Thus we can speak unambiguously of \emph{the} inverse \(u\inv\) of a
    two-sided unit \(u\). It also follows from this definition that the inverse
    of a unit is itself a unit.

    We can again alternatively characterize units in terms of the multiplication
    map \(R \to R\). We claim that an element \(u \in R\) is a left (resp.
    right) unit if and only if the left (resp. right) multiplication map \(x
    \mapsto ux\) (resp. \(x \mapsto xu\)) is surjective. We establish the claim
    for right multiplication, with the left case being analogous. Write \(\rho_u
    : R \to R\) for the right multiplication map \(x \mapsto xu\). If \(u\) is a
    right unit, then there exists a \(v \in R\) such that \(vu = 1\) and we have
    \[
        \rho_u \circ \rho_v(x) = \rho_u(xv) = xvu = x
    \]
    and thus \(\rho_v\) is a right inverse of \(\rho_u\) and hence \(\rho_u\) is
    surjective. Conversely, if \(\rho_u\) is surjective, then there exists a \(v
    \in R\) such that \(\rho_u(v) = 1\) and thus \(vu = 1\), and \(u\) is a
    right unit.

    Moreover, we can establish an important relation between units and zero
    divisors: we claim that if an element \(u \in R\) is a left (resp. right)
    unit then \(u\) is not a right (resp. left) zero divisor. We prove the case
    for right units, with the other case being analogous. Write \(\lambda_u : R
    \to R\) for the left multiplication map \(x \mapsto ux\). If \(u\) is a
    right unit, then there exists a \(v \in R\) such that \(vu = 1\), and we
    have
    \[
        \lambda_v \circ \lambda_u(x) = \lambda_v(ux) = vux = x
    \]
    and thus \(\lambda_v\) is a left inverse of \(\lambda_u\) and hence
    \(\lambda_u\) is injective. 

    The set of all units in a ring \(R\) is denoted by \(R^\times\), which forms
    a group under the multiplication of \(R\). This is called the \emph{group of
    units} of \(R\). Indeed the unity \(1\) is also a unit and is its own
    inverse and by the definition of a ring serves as the identity element of
    the group \(R^\times\). Associativity is inherited from the ring \(R\), and
    the existence of inverses (which are themselves units as remarked above)
    follows from the definition of a unit. If \(u\) and \(v\) are units in a
    ring \(R\), then there exist elements \(u\inv\) and \(v\inv\) in \(R\) such
    that \(uu\inv = u\inv u = 1\) and \(vv\inv = v\inv v = 1\). To show that the
    product \(uv\) is a also a unit, consider the product \(v\inv u\inv\) and
    observe that
    \[
        (uv)(v\inv u\inv) = u(vv\inv)u\inv =  uu\inv = 1
    \]
    and
    \[
        (v\inv u\inv)(uv) = v\inv(uu\inv)v = v\inv v = 1.
    \]
    Thus closure is satisfied and we have verified that \(R^\times\) is indeed a
    group under multiplication.

    A \emph{division ring} is  (nonzero) ring in which every nonzero element is
    a unit. A \emph{field} is a commutative division ring. The familiar sets
    \(\Q\), \(\R\) and \(\C\) are all fields, as are the sets \(\Z/p\Z\) where
    \(p\) is a prime number.  By the relation between units and zero divisors we
    have established, it follows that every field is an integral domain. The
    converse is not true: the ring \(\Z\) is an integral domain but is not a
    field since the only units in \(\Z\) are \(\pm 1\). However, both notions
    coincide in the finite case: that is, given a finite commutative ring \(R\),
    then \(R\) is a field if and only if \(R\) is an integral domain. We only
    need to show that if \(R\) is a finite integral domain, then it is a field.
    If \(a\) is not a zero divisor, then multiplication by \(a\) is injective
    and since \(R\) is finite, it must be surjective (by the pigeonhole
    principle) and thus \(a\) is a unit. Since every nonzero element of \(R\) is
    a unit, \(R\) is a field.
\end{example}

\begin{example}[Polynomial rings]
    Let \(R\) be a ring. A polynomial \(p\) in an indeterminate \(x\) with
    coefficients in \(R\) is an expression of the form
    \begin{equation}
        \label{eq:polynomial}
        p(x) = \sum_{i \geq 0} a_i x^i = a_0 + a_1 x + a_2 x^2 + \cdots
    \end{equation}
    where \(a_i \in R\) for all \(i \geq 0\) and \(a_i = 0\) for all but
    finitely many \(i\). We say that two polynomials \(p(x) = \sum a_i x^i\) and
    \(q(x) = \sum b_i x^i\) are equal if and only if their coefficients are
    equal, i.e., \(a_i = b_i\) for all \(i \geq 0\). We shall denote the set of
    all polynomials in the indeterminate \(x\) with coefficients in \(R\) by
    \(R[x]\). Now since all but finitely many of the coefficients of a
    polynomial are zero, we can alternatively write \eqref{eq:polynomial} as
    \[
        p(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n
    \]
    where \(n\) is the largest integer such that \(a_n \neq 0\). We call \(n\)
    the \emph{degree} of the polynomial \(p(x)\) and write \(\deg p = n\). The
    zero polynomial is the polynomial with all coefficients equal to zero, and
    is denoted by \(0\). The constant polynomial \(a_0\) (for any \(a_0 \in R\))
    is the polynomial with all coefficients equal to zero except for the
    constant term, which is equal to \(a_0\). The polynomial \(1\) is the
    constant polynomial with constant term equal to \(1\). The sum of two
    polynomials is defined by adding the coefficients of like terms, while the
    product of two polynomials is defined by the distributive law. That is, for
    any two polynomials \(p(x) = \sum a_i x^i\) and \(q(x) = \sum b_i x^i\) in
    \(R[x]\), we define their sum \(p(x) + q(x)\) as
    \begin{align*}
        p(x) + q(x) & = \sum (a_i + b_i) x^i = (a_0 + b_0) + (a_1 + b_1)x + \cdots\\
        &= (a_0 + a_1 x + \cdots) + (b_0 + b_1 x + \cdots) = \sum a_i x^i + \sum b_i x^i
    \end{align*}
    and their product \(p(x)q(x)\) as
    \begin{align*}
        p(x)q(x) & = \left(\sum a_i x^i\right)\left(\sum b_i x^i\right) = \sum \left(\sum_{j=0}^{i} a_j b_{i-j}\right) x^i\\
        &= a_0 b_0 + (a_0 b_1 + a_1 b_0)x + (a_0 b_2 + a_1 b_1 + a_2 b_0)x^2 + \cdots.
    \end{align*}
    With the definitions given above, we can see that \(R[x]\) is a ring under
    the operations of polynomial addition and multiplication with the zero
    polynomial as the additive identity and the constant polynomial \(1\) as the
    multiplicative identity. The ring \(R[x]\) is called the \emph{ring of
    polynomials over \(R\)}.

    Polynomial rings in more than one indeterminate can be defined by iterating
    the construction above. For example, the ring of polynomials in two
    indeterminates \(x\) and \(y\) with coefficients in a ring \(R\) is denoted
    by \(R[x, y]\) can be defined as the set of all formal sums of the form
    given in \eqref{eq:polynomial} with coefficients in \(R[x]\); i.e., \(R[x,
    y] = R[x][y]\), etc.
\end{example}

\begin{definition}
    \label{def:ring-homomorphism}
    A \emph{homomorphism of rings} is a (set) function \(\phi: R \to S\) between
    two rings \(R\) and \(S\) such that for all \(a, b \in R\),
    \begin{enumerate}[label=(\alph*)]
        \item \(\phi(a + b) = \phi(a) + \phi(b)\);
        \item \(\phi(ab) = \phi(a)\phi(b)\);
        \item \(\phi(1) = 1'\).
    \end{enumerate}
    Here the addition and multiplication on the left-hand side are those of
    \(R\), while the addition and multiplication on the right-hand side are
    those of \(S\); the unity of \(R\) is denoted by \(1\) and the unity of
    \(S\) is denoted by \(1'\).
\end{definition}

From the definition it then follows that \(\phi\) is a homomorphism of the
additive groups \(R\) and \(S\) so that \(\phi(0) = 0\) and \(\phi(-a) =
-\phi(a)\) for all \(a \in R\). However, as part of our definition of a ring
homomorphism, we require that \(\phi(1) = 1'\) so that \(\phi\) is also a
homomorphism of the monoids \((R, \cdot)\) and \((S, \cdot)\). Indeed the
condition \(\phi(1) = 1'\) does not directly follow from (a) and (b) in
Definition~\ref{def:ring-homomorphism}. For example, consider a commutative ring
\(R\) that contains an idempoent element \(e\), i.e., an element \(e\)
satisfying \(e^2 = e\). Then the function \(\phi: R \to R\) defined by \(x
\mapsto xe\) satisfies conditions (a) and (b) above, viz., \[\phi(x + y) = (x +
y)e = xe + ye = \phi(x) + \phi(y);\] we also have \(\phi(x) \phi(y) = (xe)(ye) =
(xy)e\) because \(R\) is commutative and \(e\) is idempotent and thus \(\phi(xy)
= \phi(x) \phi(y)\). However, \(\phi(1) = 1e = e\) and thus condition (c) is not
satisfied.

For any ring \(R\), the identity map \(\id_R: R \to R\) is a ring homomorphism.
Moreover, given any two ring homomorphisms \(\phi: R \to S\) and \(\psi: S \to
T\), their composition \(\psi \circ \phi: R \to T\) is also a ring homomorphism.
Thus the collection of all rings and ring homomorphisms forms a category, which
we denote by \(\Ring\). Injective, surjective and bijective ring homomorphisms
(in the sense of set functions) are likewise monomorphisms, epimorphisms and
isomorphisms in the category \(\Ring\), respectively.

As in the case of the trivial group in \(\Grp\), the zero ring is terminal in
\(\Ring\). That is, the only ring homomorphism from any ring \(R\) to the zero
ring is the trivial homomorphism that sends every element of \(R\) to the zero
element of the zero ring. However, note that the zero ring is not an initial
object in \(\Ring\): indeed given a map \(\phi: 0 \to R\) from the zero ring to
any ring \(R\), the condition \(\phi(1) = 1\) is satisfied if and only if \(R\)
is itself the zero ring. So does \(\Ring\) then have an initial object?

Given any ring \(R\), we can define the group homomorphism \(\phi: \Z \to R\) by
\(\phi(n) = n1_R\) for all \(n \in \Z\). (This is the exponential map given as
in Example~\ref{ex:exponential-maps} of Chapter~\ref{ch:groups}.) But observe
that this is also a ring homomorphism, since \(\phi(1) = 1 \cdot 1_R = 1_R\) and
\[
    \phi(mn) = (mn)1_R = m(n1_R) =^* (m1_R)\cdot(n1_R) = \phi(m)\phi(n)
\]
with the starred equality following from the distributive law. Since \(\phi\) is
determined by the condition that \(\phi(1) = 1_R\) and that \(\phi\) preserve
addition, it must be unique. Thus we have shown that \(\Z\) is initial in
\(\Ring\).

Ring homomorphisms also preserve units: that is, if \(u\) is a unit in a ring
\(R\) with inverse \(v\), then the image \(\phi(u)\) is a unit in the ring
\(\phi(R)\) with inverse \(\phi(v)\). Indeed we have
\[
    \phi(u)\phi(v) = \phi(uv) = \phi(1) = 1.
\]

\medskip

Polynomial rings satisfy a universal property similar to that of free groups we
saw in \S~\ref{sec:free-groups} of Chapter~\ref{ch:groups}. At this time we only
consider the case for the polynomial ring \(R[x_1, \ldots, x_n]\) in \(n\)
indeterminates with respect to commutative rings.

Consider a set \(X = \{\xi_1, \ldots, \xi_n\}\) of cardinality \(n\) and
consider the category \(\catg{R}_X\) whose objects are pairs \((k, R)\) where
\(R\) is a commutative ring and \(k\) is a (set) function from \(X\) to \(R\).
As in our construction of free groups, we define a morphism \((k_1, R_1) \to
(k_2, R_2)\) in \(\catg{R}_X\) to be a commutative diagram
\[
    \begin{tikzcd}
        R_1 \ar[r, "\phi"] & R_2\\
         X \ar[u, "k_1"] \ar[ur, "k_2"'] &
    \end{tikzcd}
\]
where \(\phi\) is a ring homomorphism.

\begin{theorem}
    With the notation as above, let \[i: X \to \Z[x_1, \ldots, x_n]\] be the map
    that sends each \(\xi_i\) to the indeterminate \(x_i\). Then the pair \((i,
    \Z[x_1, \ldots, x_n])\) is an initial object in \(\catg{R}_X\).
\end{theorem}

\begin{proof}
    To prove that \((i, \Z[x_1, \ldots, x_n])\) is an initial object in
    \(\catg{R}_X\), we must show that for any object \((k, R)\) in
    \(\catg{R}_X\), there exists a unique ring homomorphism \(\phi: \Z[x_1,
    \ldots, x_n] \to R\) such that the diagram
    \[
        \begin{tikzcd}
            \Z[x_1, \ldots, x_n] \ar[r, "\phi"] & R\\
            X \ar[u, "i"] \ar[ur, "k"']
        \end{tikzcd}
    \]
    commutes. Now if \(\phi\) makes the above diagram commute, we must have
    \(\phi(x_i) = (\phi \circ i)(\xi_i) = k(\xi_i)\). By defining \(\phi\) to be
    precisely this map, it remains for us to show that \(\phi\) is a unique ring
    homomorphism, in which case we must have
    \begin{align*}
        \phi\left(\sum_{i_1, \ldots, i_n} a_{i_1, \ldots, i_n} x_1^{i_1} \cdots x_n^{i_n}\right) &= \sum_{i_1, \ldots, i_n} \phi(a_{i_1, \ldots, i_n}) \phi(x_1)^{i_1} \cdots \phi(x_n)^{i_n}\\
        &= \sum_{i_1, \ldots, i_n} \iota(a_{i_1, \ldots, i_n}) k(\xi_1)^{i_1} \cdots k(\xi_n)^{i_n}
    \end{align*}
    where \(\iota: \Z \to R\) is the unique ring homomorphism that sends each
    integer to the corresponding element of \(R\) (since \(\Z\) is initial in
    \(\Ring\)). Thus \(\phi\) is uniquely determined by the condition that
    \(\phi(x_i) = k(\xi_i)\) for all \(i\), preserves addition and
    multiplication, and sends the unity of \(\Z[x_1, \ldots, x_n]\) to the unity
    of \(R\) and is therefore a ring homomorphism.
\end{proof}


For \(n = 1\), the universal property tells us that if \(r\) is an element of
any ring \(R\), then there exists a unique ring homomorphism \(\phi: \Z[x] \to
R\) such that \(\phi(x) = r\). That is, \(\phi\) `extends' the canonical
embedding \(\iota: \Z \to R\) (since \(Z\) is initial in \(\Ring\)) and we have
\(\iota = \phi{|_{\Z}}\). Since each \(p \in \Z[x]\) can be written uniquely as
a sum of the form \(\sum a_i x^i\), we can write \(\phi(p) = \sum a_i r^i\); but
then this latter sum is well-defined even if \(R\) is not commutative. More
generally, if we let \(\alpha: R \to S\) be a fixed ring homomorphism and let
\(s \in S\) be an element of \(S\) commuting with \(\alpha(r)\) for all \(r \in
R\), then there exists a unique ring homomorphism \(\psi: R[x] \to S\) that
extends \(\alpha\) and sends \(x\) to \(s\). In particular, we can define the
\emph{evaluation homomorphism} \(\ev_r: R[x] \to R\) for any \(r \in R\) by 
\[
    \ev_r(p) = p(r) = \sum_{i \geq 0} a_i r^i.
\]
This can ve viewed as \(\psi(p(x))\) with \(\psi\) obtained as above by taking
\(\alpha = \id_R\) and \(s = r\). Thus every polynomial \(p(x)\) determines a
\emph{polynomial function} \(R \to R\) that sends each element \(r \in R\) to
\(p(r)\).

We can similarly define the kernel of a ring homomorphism \(\phi: R \to S\) as
the set
\[
    \ker \phi = \{r \in R : \phi(r) = 0\};
\]
that is, the kernel of \(\phi\) is kernel of the group homomorphism \(\phi: R
\to S\), with both rings viewed as additive groups. Kernels are (normal)
subgroups of the additive group \(R\) as we have shown in
Chapter~\ref{ch:groups} and play a similarly important role in the theory of
rings, as we shall later explore in detail in \S~\ref{sec:ideals}. In the
meantime, we present the following result, analogous to
Theorem~\ref{thm:kernel-injective-grp} of Chapter~\ref{ch:groups}.

\begin{theorem}
    \label{thm:kernel-injective-ring}
    Let \(\phi: R \to S\) be a ring homomorphism. The following statements are
    equivalent:
    \begin{enumerate}[label=(\alph*)]
        \item \(\phi\) is injective as a map between sets.
        \item \(\phi\) is a monomorphism in \(\Ring\).
        \item The kernel of \(\phi\) is \(\{0\}\).
    \end{enumerate}
\end{theorem}

\begin{proof}\(\)

    \begin{enumerate}[wide]
        \item[(a) \(\implies\) (b).] If \(\phi\) is injective as a map between
        sets, then it must be a monomorphism in \(\Set\); i.e., for any set
        \(X\) and any functions \(\alpha, \beta: X \to R\) such that \(\phi
        \circ \alpha = \phi \circ \beta\), we must have \(\alpha = \beta\). In
        particular, if \(X\) is a ring and \(\alpha, \beta: X \to R\) are ring
        homomorphisms, then \(\phi \circ \alpha = \phi \circ \beta\) implies
        that \(\alpha = \beta\), so that \(\phi\) is a monomorphism in
        \(\Ring\).
        
        \item[(b) \(\implies\) (c).] Let \(\phi\) is a monomorphism in \(\Ring\)
        and suppose that \(r \in \ker \phi\). The universal property of
        integer-valued polynomial rings allow us to define unique ring
        homomorphisms \(\ev_r : \Z[x] \to R\) and \(\ev_0: \Z[x] \to R\) such
        that \(\ev_r(x) = r\) and \(\ev_0(x) = 0\), respectively. This gives us
        the diagram
        \[
            \begin{tikzcd}
                \Z[x]\arrow[r,shift left=2pt,"\ev_r"]\arrow[r,shift right=2pt,"\ev_0", swap] &  R \arrow[r,"\phi"] & S;
            \end{tikzcd}
        \]
        but since \(r \in \ker \phi\), we have \(\phi(r) = 0 = \phi(0)\) and
        thus \(\phi \circ \ev_r = \phi \circ \ev_0\). Since \(\phi\) is a
        monomorphism, we must have \(\ev_r = \ev_0\) and thus \(r = 0\), so that
        \(\ker \phi = \{0\}\).

        \item[(c) \(\implies\) (a).] If \(\ker \phi = \{0\}\), then for any \(r,
        s \in R\) such that \(\phi(r) = \phi(s)\), we have \(\phi(r - s) = 0\)
        and thus \(r - s = 0\), so that \(r = s\). Thus \(\phi\) is injective as
        a map between sets.
    \end{enumerate}
\end{proof}

By the above theorem, we see that if \(R \to S\) is a monomorphism, then \(R\)
may be identified with a subset of \(S\). We can thus formalize this notion
using the following definition, analogous to Definition~\ref{def:subgroup} of
Chapter~\ref{ch:groups}.

\begin{definition}
    A \emph{subring} of a ring \(R\) is a subset \(S \subset R\) such that the
    inclusion map \(\iota: S \to R\) is a ring homomorphism. 
\end{definition}

As in \(\Grp\), this is equivalent to saying that \(G\) is a subring if it
contains the unity of \(R\) and is itself a ring under the operations of
addition and multiplication in \(R\) (restricted to \(S\)). By this definition,
the zero ring is not a subring of any nonzero ring, since the zero ring does not
contain the unity of any nonzero ring.

Our exposition has thus far established important parallels between the theory
of rings and that of groups (as well as their corresponding categories) but we
now note one major difference. We have remarked that injective, surjective and
bijective ring homomorphisms are monomorphisms, epimorphisms and isomorphisms in
\(\Ring\), respectively. In Theorem~\ref{thm:kernel-injective-ring} we have
established that injectivity and monicity are equivalent notions in \(\Ring\);
we shall now show however that no such equivalence holds for surjective
functions and epimorphisms in \(\Ring\). Now a surjective ring homomorphism is
an epimorphism in \(\Ring\) since it is already an epimorphism in \(\Set\)
(following an analogous argument as the implication (a) \(\implies\) (b) in the
proof of Theorem~\ref{thm:kernel-injective-ring}). However, the converse is not
true in \(\Ring\): i.e., an epimorphism in \(\Ring\) need not be surjective. To
see this, consider the inclusion map \(\iota: \Z \to \Q\) of the integers into
the rationals which is clearly not a surjective map between sets. However, we
shall show that \(\iota\) is an epimorphism in \(\Ring\).

Consider the diagram
\[
    \begin{tikzcd}
        \Z \arrow[r,"\iota"] & \Q \arrow[r,shift left=2pt,"\alpha_1"]\arrow[r,shift right=2pt,"\alpha_2", swap] &  R
    \end{tikzcd}
\]
where \(R\) is any ring and \(\alpha_1, \alpha_2: \Q \to R\) are ring
homomorphisms that agree on \(\Z\) (which we can guarantee because \(\Z\) is
initial); that is, \(\alpha_1 \circ \iota = \alpha_2 \circ \iota\). Write
\(\alpha\) for this common value. Then for any \(p, q \in \Z\) with \(q \neq
0\), we have
\[
    \alpha_k\left(\frac{p}{q}\right) = \alpha_k(p)\alpha_k(q\inv) = \alpha(p)\alpha(q)\inv
\]
for \(k = 1, 2\). But then \(\alpha_1 = \alpha_2\) and thus \(\iota\) is an
epimorphism in \(\Ring\). Thus we see that the category \(\Ring\) does not
satisfy the epimorphism axiom of the category \(\Set\), which states that a
morphism is an epimorphism if and only if it is surjective. For this, it also
follows that a ring homomorphism that is both a monomorphism and an epimorphism
is not necessarily an isomorphism since satisfying both conditions does not
guarantee that the homomorphism is bijective and thus an isomorphism.

\bigskip

We end this section by reviewing the ring \(\End_{\Ab}(G)\) of endomorphisms of
an abelian group \(G\) we have introduced at the start of this chapter with the
operations of pointwise addition and composition. In particular, for \(G = \Z\),
we have the following result:

\begin{theorem}
    The ring \(\End_{\Ab}(\Z)\) is isomorphic to the ring \(\Z\).
\end{theorem}

\begin{proof}
    
\end{proof}

\section{Ideals and quotient rings}
\label{sec:ideals}

We have studied normal subgroups and their equivalence to kernels of group
homomorphisms in Chapter~\ref{ch:groups}. We now introduce a similar concept in
the context of rings.

\begin{definition}[Ideal]
    \label{def:ideal}
    Let \(R\) be a ring. A subgroup \(\idl{a}\) of the additive group of \(R\)
    is called a left (resp. right) \emph{ideal} of \(R\) if for all \(r \in R\)
    we have \(r\idl{a} \subset \idl{a}\) (resp. \(\idl{a}r \subset \idl{a}\)).
    That is, for all \(r \in R\) and \(a \in \idl{a}\), we have \(ra \in
    \idl{a}\) (resp. \(ar \in \idl{a}\)). An ideal that is both a left and right
    ideal is called a \emph{two-sided ideal} or simply an \emph{ideal}.
\end{definition}

As expected, if \(R\) is a commutative ring, then every left ideal is a right
ideal and vice versa, so that every ideal is a two-sided ideal. For the
remainder of this section, we consider only commutative rings and thus refer to
ideals without classification, unless otherwise stated. Analogously, we see that
the kernel of any ring homomorphism \(\phi: R \to S\) is an ideal of \(R\).
Indeed, if \(\phi: R \to S\) is a ring homomorphism, then the kernel of \(\phi\)
is a subgroup of the additive group of \(R\), and for any \(r \in R\) and \(a
\in \ker \phi\), we have \(\phi(ra) = \phi(r)\phi(a) = \phi(r)0 = 0\) and thus
\(ra \in \ker \phi\). (A parallel argument shows that \(\ker \phi\) is a right
ideal of \(R\).) The zero ideal \(\{0\}\) is the trivial subgroup of the
additive group of \(R\) and can be routinely shown to be an ideal of \(R\).

Since the additive group \(R\) is abelian, every subgroup \(\idl{a}\) is
necessarily normal and thus we can define the quotient group \(R/\idl{a}\) whose
elements are cosets of \(\idl{a}\), i.e., the sets \(r + \idl{a}\) for all \(r
\in R\) (written in additive notation). We can likewise define the canonical
projection \(\pi: R \to R/\idl{a}\) that sends each element \(r \in R\) to its
coset \(r + \idl{a}\). This is a surjective group homomorphism with kernel
\(\idl{a}\) (Theorem~\ref{thm:induced-homomorphism-props}, Ch.~\ref{ch:groups}).
Moreover, \(R/\idl{a}\) is itself a group under the operation \((r + \idl{a}) +
(s + \idl{a}) = (r + s) + \idl{a}\) (Theorem~\ref{thm:quotient-group},
Ch.~\ref{ch:groups}, this time in additive notation). We shall now want to ask
if we can define a ring structure on \(R/\idl{a}\) that makes \(\pi: R \to
R/\idl{a}\) a ring homomorphism.

As we have already defined an addition operation on \(R/\idl{a}\), we shall now
define a multiplication operation that induces a unity element and is
distributive with respect to addition. But first note that since \(R\) is
abelian, then we have, for all \(r, s \in R\),
\[
    (r + \idl{a})+(s + \idl{a}) = (r + s) + \idl{a} = (s + r) + \idl{a} = (s + \idl{a})+(r + \idl{a}),
\]
so that the addition we have defined in \(R/\idl{a}\) is commutative. For
\(\pi\) to be a ring homomorphism, the multiplication in \(R/\idl{a}\) must
follow, for all \(r, s \in R\),
\[
    (r + \idl{a})(s + \idl{a}) = \pi(r)\pi(s) =^* \pi(rs) = rs + \idl{a},
\]
with the starred equality holding because we require that \(\pi\) be a ring
homomorphism. Thus the only sensible definition of multiplication in
\(R/\idl{a}\) is
\[
    (r + \idl{a})(s + \idl{a}) = rs + \idl{a}.
\]
This operation is well-defined because if \(r + \idl{a} = r' + \idl{a}\) and \(s
+ \idl{a} = s' + \idl{a}\), then \(r - r' \in \idl{a}\) and \(s - s' \in
\idl{a}\) so that
\[
    rs - r's' = rs - rs' + rs' - r's' = r(s - s') + (r - r')s' \in \idl{a}
\]
from which it follows that \(rs + \idl{a} = r's' + \idl{a}\). Associativity
follows from the associativity of multiplication in \(R\), and the unity element
of \(R/\idl{a}\) is the coset \(1 + \idl{a}\) where \(1\) is the unity element
of \(R\). The distributive law follows from the distributive law in \(R\), and
thus we have defined a ring structure on \(R/\idl{a}\) that makes \(\pi: R \to
R/\idl{a}\) a ring homomorphism. We call \(R/\idl{a}\) the \emph{quotient ring}
of \(R\) modulo the ideal \(\idl{a}\).

The universal property of quotients holds and is analogous to the result for
groups. We reproduce the statement here for completeness.

\begin{theorem}
    \label{thm:ideal-universal-property}
    Let \(R\) be a ring and \(\idl{a}\) be an ideal of \(R\). Then for any ring
    \(S\) and any ring homomorphism \(\phi: R \to S\) such that \(\idl{a}
    \subset \ker \phi\), there exists a unique ring homomorphism
    \(\overline{\phi}: R/\idl{a} \to S\) such that the diagram
    \[
        \begin{tikzcd}
            R \ar[r, "\phi"] & S\\
            R/\idl{a} \ar[u, "\pi"] \ar[ur, "\overline{\phi}"']
        \end{tikzcd}
    \]
    commutes. Here \(\pi: R \to R/\idl{a}\) is the canonical projection.
\end{theorem}

As before, the induced homomorphism \(\overline{\phi}\) is defined by
\(\overline{\phi}(r + \idl{a}) = \phi(r)\) for all \(r \in R\). The proof of
this theorem is analogous to that of Theorem~\ref{thm:induced-homomorphism} of
Chapter~\ref{ch:groups} and is thus left out here. Continuing our analogy with
groups, we have the following result, which gives us the canonical decomposition
of any ring homomorphism (cf. Theorem~\ref{thm:canonical-decomposition-grp} of
Chapter~\ref{ch:groups}).

\begin{theorem}
    \label{thm:canonical-decomposition-ring}
    Every ring homomorphism \(\phi: R \to S\) may be decomposed as follows:
    \[
        \begin{tikzcd}
            R \arrow[r, two heads] \arrow[rrr, bend left, "\phi"]   & R/\ker\phi \arrow[r,"\sim" above, "\overline{\phi}" below]   & \img\phi \arrow[r, hook]  & S,
        \end{tikzcd}
    \]
    where \(\overline{\phi}\) is the induced isomorphism in the preceding
    theorem.
\end{theorem}

\section{Modules}

\section{Complexes and homology}